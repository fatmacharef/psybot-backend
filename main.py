from fastapi import FastAPI
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
from pydantic import BaseModel
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from duckduckgo_search import DDGS
import os
from huggingface_hub import login

# ðŸ“Œ Charger le token Hugging Face depuis la variable dâ€™environnement
HF_TOKEN = os.getenv("HF_TOKEN")

if HF_TOKEN:
    login(HF_TOKEN)

# ðŸ“Œ Charger le modÃ¨le GPT et le tokenizer
model_name = "fatmata/psybot"  # Remplace par ton modÃ¨le
tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=HF_TOKEN)
model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=HF_TOKEN)

# ðŸ“Œ Charger le modÃ¨le BERT pour lâ€™analyse des Ã©motions
emotion_classifier = pipeline("text-classification", model="monologg/bert-base-cased-goemotions-original")
analyzer = SentimentIntensityAnalyzer()

# ðŸ“Œ DÃ©finition de lâ€™API FastAPI
app = FastAPI()

# ðŸ“Œ ModÃ¨le pour recevoir l'entrÃ©e utilisateur
class UserInput(BaseModel):
    user_input: str

# ðŸ“Œ Fonction pour gÃ©nÃ©rer une rÃ©ponse avec GPT
def generate_response(user_input):
    prompt = f"<|startoftext|><|user|> {user_input} <|bot|>"
    inputs = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)

    with torch.no_grad():
        output = model.generate(
            inputs,
            max_new_tokens=100,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
            do_sample=True,
            temperature=0.7,
            top_k=50,
            top_p=0.9,
            repetition_penalty=1.2
        )

    response = tokenizer.decode(output[0], skip_special_tokens=True)
    return response.split("<|bot|>")[-1].strip()

# ðŸ“Œ Endpoint principal de l'API
@app.post("/chat/")
async def chat_with_bot(user_input: UserInput):
    response = generate_response(user_input.user_input)
    return {"response": response}

